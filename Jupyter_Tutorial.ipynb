{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Python with Jupyter Notebooks!\n",
    "\n",
    "\n",
    "##### In this course we will go over some of the basics in getting started with the python language in a Jupyter Notebook and cover some basics in programming followed by some examples!\n",
    "\n",
    "\n",
    "## What is Programming and How Will it Help My Role?\n",
    "\n",
    "\n",
    "#### Programming is becoming increasingly popular and needed for many engineering roles as the amount of tasks required to complete in an expeditious manner is skyrocketting at the same time we are experiencing a data overload from various sources such as IoT (Internet of Things), Manufacturing Measurement Equipment, and Large Databases. Some times this data is a huge pool collected in a data lake for sifting through and other times this data is high velocity with the need to handle and react quickly with one or more sources. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Lake](\\Data_lake_Data_Warehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So when we see this we can see that many of our systems feed into what we can call a \"Data Lake\" that is the primary place where we are interacting with our data. There are other efforts that are ongoing which may also allow us to access a much more structered set of data in a \"Data Warehouse\" which would be something a little more akin to Denodo or even higher levels of structure or accessibility. ####\n",
    "\n",
    "### What is the Key here is that when utilizing programming languages such as Python we are not bound or restricted to accessing the Data at any of these points. Currently with our standard practices of using only SQL Queries we are isolated to only collecting data from the Data Lakes and with a limited array of analysis tools to do this. ###\n",
    "##### This of course excludes any manual or non-automated data pulling we may do directly in each application which is usually much more restrictive and not analysis friendly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
